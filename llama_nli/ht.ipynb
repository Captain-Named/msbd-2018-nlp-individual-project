{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForSequenceClassification, AutoTokenizer, Trainer, DataCollatorWithPadding\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gpt3_text', 'wiki_bio_text', 'gpt3_sentences', 'annotation', 'wiki_bio_test_idx', 'gpt3_text_samples'],\n",
       "    num_rows: 238\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = load_dataset(\"./datasets--potsawee--wiki_bio_gpt3_hallucination\", trust_remote_code=True)\n",
    "dataset = datasets[\"test\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['hypothesis', 'premise', 'label'],\n",
       "    num_rows: 1908\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = {}\n",
    "hypothesis = []\n",
    "premise=[]\n",
    "label=[]\n",
    "for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[\"gpt3_sentences\"][i])):\n",
    "        hypothesis.append(dataset[\"gpt3_sentences\"][i][j])\n",
    "        premise.append(dataset[\"wiki_bio_text\"][i])\n",
    "        label.append(dataset[\"annotation\"][i][j])\n",
    "\n",
    "dataset_dict[\"hypothesis\"] = hypothesis\n",
    "dataset_dict[\"premise\"] = premise\n",
    "dataset_dict[\"label\"] = label\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./llama-3.2-1B/ and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_fast.PreTrainedTokenizerFast'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61848bc8a14441cebbce51721745db72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1908 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1908\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./llama-3.2-1B/\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "model = LlamaForSequenceClassification.from_pretrained(\"./llama-3.2-1B/\", num_labels=3) # bnb\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "p_model = PeftModel.from_pretrained(model, model_id=\"./checkpoints/checkpoint-24471\")\n",
    "merge_model = p_model.merge_and_unload()\n",
    "\n",
    "str2int = {'major_inaccurate': 2, 'accurate': 0, 'minor_inaccurate': 1}\n",
    "\n",
    "print(type(tokenizer))\n",
    "def process_function(example):\n",
    "    prompt = f\"This is an NLI task. I'll give you two sentences which are the premise and the hypothesis respectively.\\n \\\n",
    "              If the hypothesis can be inferred from the premise, the answer is entailment.\\n \\\n",
    "              If the hypothesis is inconsistent with the premise, the answer is contradiction.\\n \\\n",
    "              If the hypothesis is unrelated to premise, the answer is neutral.\\n\\n \\\n",
    "              ---\\n \\\n",
    "              Important Note: You can only response ONE word among entailment, neutral and contradiction.\\n \\\n",
    "              ---\\n\\n \\\n",
    "              The premise: { example['premise'] }\\n \\\n",
    "              The hypothesis: { example['hypothesis'] }\\n \\\n",
    "              The answer: \"\n",
    "    \n",
    "    tokenized_example = tokenizer(text=prompt, max_length=256, padding=\"max_length\")\n",
    "    tokenized_example[\"labels\"] = str2int[example[\"label\"]]\n",
    "    return tokenized_example\n",
    "\n",
    "tokenized_dataset = dataset.map(process_function, batched=False, remove_columns=dataset.column_names)\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2509/2709756555.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(model=merge_model,\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7751572327044025\n",
      "precision: 0.8085106382978723\n",
      "recall: 0.22093023255813954\n",
      "f1: 0.3470319634703196\n"
     ]
    }
   ],
   "source": [
    "# use trainer as a predictor\n",
    "\n",
    "trainer = Trainer(model=merge_model,\n",
    "                  tokenizer=tokenizer,\n",
    "                  data_collator=DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "                  )\n",
    "\n",
    "predictions = trainer.predict(tokenized_dataset)   #1\n",
    "\n",
    "predicted_labelsint = [prediction.argmax(axis=-1) for prediction in predictions.predictions]\n",
    "\n",
    "\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fp = 0\n",
    "fn = 0  # p:0, n:1/2\n",
    "for pred_label, true_label in zip(predicted_labelsint, tokenized_dataset['labels']):\n",
    "    if pred_label in [1, 2] and true_label in [1, 2]:   # non-factual for (0.5 and 1)\n",
    "        tn = tn + 1\n",
    "    elif pred_label == true_label:  # both 0, factual\n",
    "        tp = tp + 1\n",
    "    elif true_label == 0:\n",
    "        fn = fn + 1\n",
    "    else:   # true_label != 0\n",
    "        fp = fp + 1\n",
    "precision = float(tp)/(tp+fp)\n",
    "recall = float(tp)/(tp+fn)\n",
    "f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "acc = float(tp+tn)/len(predicted_labelsint)\n",
    "print(f\"tn={tn}, tp={tp}, fn={fn}, fp={fp}\")\n",
    "print(f\"acc: {acc}\")\n",
    "print(f\"precision: {precision}\")\n",
    "print(f\"recall: {recall}\")\n",
    "print(f\"f1: {f1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
